SUS-14-022 PAS v17

Questions coming up while reading:

Tau ID in fastsim: Do you apply any additional uncertainties w.r.t. the POG recommendation for fullsim?

Answer: It was discussed previously. Needs link to discussion.

130: Can you motivate why you set m^vis = 0? This doesn't strike me as necessary since it's by definition the visible mass, but maybe I'm simply missing the point.
Or doesn't this simply mean that you set the lepton mass to zero, which is just a fair approximation? In this case, it might be less confusing to simply call it m^\ell.

10: Clean up references 6-15: a) add relevant ATLAS references (!), b) use latest available searches, c) get rid of outdated/unnecessary results
13: [16-18] Again should add relevant ATLAS references (still make the point that this search is missing in CMS)


111: You write you normalise most processes to NLO cross sections and provide according references while, for many processes, in particular ttbar, a full NNLO calculation is available. Please update the sentence, the references, and the used cross sections.


156-: The text would profit from first explaining why you need to search regions and then laying out these search regions

Figure 2:
H- Please put the plots in as vector graphics (!!)
H- Change the order in the legend such that it corresponds to the order in the plot, or vice versa
H- QCD -> QCD multijet
H- Try to increase all font sizes
- Define and explain the error band in the ratio
- Consider to add the error band also to the main figures to substantiate your statement that observation and prediction agree within statistical uncertainties


I think it would be a good idea to add tables with the exact selection requirements (of course also in section 4)

H-Figure 3: Please apply all recommendations I sent for figure 2 as well here. Also for both figures, remove one significant digit from the luminosity number





6.2: The first paragraph is written a bit sloppily "(DPhi, lep veto, )".
First paragraph + 232-240: I'm confused by the description. The second paragraph seems to repeat what is written in the paragraph. I suggest you try to rewrite this more clearly before we give more detailed comments.


243: Can you give the values and uncertainties of the efficiencies obtained in data and simulation? Unlike for the normalisation, you don't write that these are compatible, so it's not clear why you'd choose the value obtained in data in something you call a "validation region" - it rather seems to be part of your data-driven method.

Table 2: Suddenly an uncertainty appears that doesn't seem to be described in the text, "sys. shape", which according to the caption "  .       e estimation found from varying the Ď"had energy scale within its uncertainty. The â?osys. shapeâ?ť takes into account the difference between the shape of the search variable distribution in data and Monte Carlo." How is this derived? Where is it discussed? If it's the result of the mu-tau_h validation, this isn't really a shape uncertainty (except if you call two bins a shape) but the very same efficiency that you're discussing all the time (?)



263: What's the size of this correction? I would give the number.
268/269: This reads like a completely arbitrary uncertainty. Please motivate this better (there will certainly be statistical uncertainties, uncertainties in the MC estimation of the OS-SS correction, dependence on tau pT, which has an effect on your final cut variables, etc)
271: Also here, I would prefer to give the numbers so we can see to what extent it closes (1%? 10%? 50%?)
278: One small difference are the different anti-muon/anti-electron discriminators, or aren't they? From lines 92-93 and section 5, it's not clear what exactly you do. I suggest to write exactly what you do there.


Table 5:
- tau energy scale: It's not clear to me why the tau energy scale uncertainty of 3% per tau happens to have the exact same effect for signal and all simulated backgrounds, and also the same effect for events with one and two hadronic taus. Can you elaborate on this? Is this what you put into the datacards? Otherwise I would have thought that a range of values would be more reasonable.
- I would also find it useful if you could indicate in the table which uncertainties are shape-altering
- For pileup, ptmiss and MC statistics I would also rather expect a range of values, or do you really put these values in? If the latter, how can the MC statistics uncertainty be the exact same number in all signal regions for all backgrounds?
- I would find it more useful to at least separate by simulated backgrounds and backgrounds estimated from data
- Total: Is this simply the quadratic sum?

288-289: What exactly do you mean by "accessible"? I think that also the model-independent cross section limits have some value regardless of the exact parameters, so it's maybe a too drastic shortcut to use just one value, even if conservative
291: You *may* want to always write "uncertainty in" instead of "on" to comply with the PubCom guidelines
291-292: Add a reference if you haven't measured these yourselves; otherwise add a brief statement how you did it and add a reference for tag and probe in case
293-296: I think you should use the H->tau tau reference, or the TAU-14-001 paper, to justify these uncertainties since you haven't measured them yourself
297-299: Add reference for b-tag uncertainties
300-302: Add proper reference to inelastic xsec + uncertainties
307: remove "which is a matrix-element generator" - this is trivial and doesn't help to make your point (both are actually)
310-311: I guess what you want to say is that these are multi-leg generators (?) and that the uncertainties are covered by scale variations (??). Like currently written, it doesn't make much sense.
312-319: Is there a citable reference for this procedure? please add if yes
312: Technically, you should rather group uncertainties by sources since these should be correlated in the final model. Just consider the case of the hadronic tau scale uncertainty: When varying it, you should calculate the effect on ptmiss at the same time and use this as one nuisance parameter. If the other parameters do not appear as additional single uncertainties, it's of course ok to add them in quadrature to make the total list of uncertainties shorter. From the text, it's not perfectly clear what you actually do - I guess you're even doing the right thing. So maybe you want to improve the description such that it's clear that ptmiss-related uncertainties can arise from a number of sources, some of which are already described above (tau and lepton energy scales), some of which are negligible, and some of which lead to the 5% uncertainty (which one is it actually?)
320-322: "are taken to be" sounds weird - you can strictly calculate these numbers. Why do you make the shortcut to put these to 10% and 20% even though you have completely different samples, and what are the actual underlying numbers?
323: Give a reference to these differences.
323-324: You clearly cannot say that the fast simulation is "insufficient". I would just say that there are differences between the fast and full simulation, and that these give rise to the uncertainties you mention.
328: The connection is not logical: If you have very few remaining events, this should rather give rise to a properly calculated large uncertainty of statistical origin, and not to an increased theory uncertainty. The latter may well be required in addition to the statistical uncertainty because of the larger effect of scale variations in the considered phase space. 50% may be an adequate choice given that these are very small backgrounds.
331: Do you really add all uncertainties in quadrature for the final fit? This would be wrong since you have different signal categories that you fit at the same time. Can you elaborate a bit on this and let us know whether you correlate the uncertainties across signal regions?



Table 6:
- VV is undefeind
- "Fake" is not a good name for a sample, in particular if it doesn't include multijet production! (a comment that applies also in other parts of the paper draft)
- Caption: - mention which uncertainties are shown for the predictions, i.e. what are the first and second uncertainty numbers (ideally in the table itself as well) - I would refrain from repeating here to which degree the different backgrounds are estimated from simulation/data/mixture/... - in particular, remove "but the 'Fake' for the tt channel is not completely data driven" which sounds awkward. You could e.g. write that the largest backgrounds (the "Fake" and QCD contributions) are derived from data as described above.


377: ATLAS sets explicit limits on pairs of left- and right-handed staus. Can you comment on why you don't do it here? This should certainly be mentioned in the text.

Section 9:

I will not comment on the sentences and language in section 9 since I don't fully understand to which extent the approach chosen here makes sense. My most important questions is whether the implicit assumption is justified that the signal and background uncertainties in the 4 signal regions are completely uncorrelated... I am rather used to giving out 2D excluded cross sections for different model parameters, which have very similar information. You could still give out section 9 but may move it to an appendix.

Section 10:

At the beginning of the section, a quick recapitulation of the search strategy similar to the abstract would be good.


Please add a statement also on the direct stau production - may be as simple as stating that you do set limits as a function of stau mass. Also add that you provide the limits in a model-independent way, if you choose to do so.

 Commnets from Arun:


1. QCD bkg estimation in sec 6.1: First of all the control regions in Fig.4
is really confusing. From the definition of the control regions (II and
III) it seems like at least one tau_h pass the loose tau isolation while
the other may or may not pass the loose isolation. Actually both tau_h pass
loose isolation. I would suggest it to redefine or, at least add,
loose-loose, loose-medium, medium-medium as you have in AN.

 Given that the control regions have both tau_h candidates passing loose
isolation (medium iso for CR-I), it may have significant contribution from
W+jets events. What is the level of W+jets contamination in these control
regions? If it is significant, how do you trust MC to subtract W+jets
contribution? How many MC W+jets events (entries) survive in the control
regions?

How is the QCD events obtained from data to measure the efficiency of the
deltaphi cut? Could you add a few line of text in AN and PAS?
Is it SS events with all signal like selections and with/without DeltaPhi
cut?
What is the statistics of events with these selections and what is fraction
of other background contamination?

2. Table.3 in PAS: Starting from the same DY MC sample for all signal
region in this table, how do you get significantly smaller statistical
uncertainty for SR1 relative to other 3 regions? How many MC events
(entries) do you have in this bin? Given that the yield in SR1 and SR2 are
not significantly different, I would have expected their relative
statistical uncertainty to be similar.

Again the validation of MC is performed in a control region which is far
away from the signal selection. Is it possible to find some control region
closer to the signal selection?
I think, one needs to validate the MT2 and SumMT(tau) distributions in a
control region where other cuts could be reversed.

3. sec. 6.1.2 in AN: you mention that events in all bins (except bin-1) in
Fig.8 are independent. Does it mean that no events which pass (fail) lepton
veto (in bin 2,3) will pass (fail) DeltaPhi cut (bin 4,5)?
Do you understand why the estimation in inverse Z veto and lepton veto are
quite off from the rest of the bins? In SR2 region, the fit value seems to
be strongly biased by these two bins as it has smaller uncertainty than the
others. I would have expected to exclude the last bin from the fit.

4. sec.6.4: What is the number of initial events ( N_loose and N_NTight in
eq.6 ) before applying the fake rate method? Given the fake probability and
the final fake estimation, I guess, the N_loose is of the order of 10
events.
I am wondering whether it is ok to apply a probabilistic method starting
with a small number of events. Should not the statistics in the loose
category be increased significantly by loosening further the isolation cut
on tau candidates?



